{
 "cells": [
  {
   "cell_type": "code",
   "id": "aa93539a31898f46",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "753a20cbfc5a8eef",
   "metadata": {},
   "source": [
    "# Selfies Featurization with one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "e54625c9a75c2435",
   "metadata": {},
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import selfies as sf\n",
    "from tqdm import tqdm\n",
    "from selfies import EncoderError\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "from collections import deque\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from Code.Utils.util_methods import NNUtils"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "current_file = os.getcwd()  # or provide a specific file path\n",
    "try:\n",
    "    base = NNUtils.find_project_root(current_file)\n",
    "    print(f\"Project root found: {base}\")\n",
    "except FileNotFoundError as e:\n",
    "    base=None\n",
    "    print(e)\n",
    "    \n",
    "load_dotenv(f'{base}/.env')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b666e71f8723423",
   "metadata": {},
   "source": [
    "# Todo: Set the variables False if not needed\n",
    "NEW_ORDERED_DICT = True # If a new ordered (by SELFIES part occurence) dictionary should be used, where only the present SELFIES parts are used, instead of the existing dict\n",
    "NEW_RANDOM_DICT = False # Same but the order of the parts will be random\n",
    "NORMALIZE_SPECTRA = False # KEEP IT FALSE # If the spectra (X data) should be normalized between 0 and 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8466ba3fb85bf8b1",
   "metadata": {},
   "source": [
    "## Load the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "id": "bfbc8e846bad690f",
   "metadata": {},
   "source": [
    "cleaned_df = NNUtils.read_big_csv(f\"{base}/Dataset/Mass_spectra/cleaned_df.csv\")\n",
    "print(cleaned_df.shape)\n",
    "cleaned_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "672e34fbbb1570ba",
   "metadata": {},
   "source": [
    "## Add the selfies "
   ]
  },
  {
   "cell_type": "code",
   "id": "cc2b831b4995cef6",
   "metadata": {},
   "source": [
    "cleaned_df_selfies = cleaned_df.copy()\n",
    "\n",
    "# add a new column with SELFIES representation\n",
    "cleaned_df_selfies = NNUtils.add_selfies_to_df(cleaned_df_selfies)\n",
    "\n",
    "cleaned_df_selfies.drop(cleaned_df_selfies[cleaned_df_selfies[\"SELFIES\"] == \"Invalid\"].index, inplace=True)\n",
    "cleaned_df_selfies.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# filter rows where SELFIES contains a dot\n",
    "contains_dot = cleaned_df_selfies[\"SELFIES\"].str.contains(r'\\.')\n",
    "# count the number of SELFIES containing a dot\n",
    "count_with_dot = contains_dot.sum()\n",
    "print(f\"Number of SELFIES containing a dot: {count_with_dot}\")\n",
    "# remove rows with SELFIES containing a dot\n",
    "cleaned_df_selfies = cleaned_df_selfies[~cleaned_df_selfies[\"SELFIES\"].str.contains(r'\\.')].reset_index(drop=True)\n",
    "\n",
    "\n",
    "print('Invalid SMILES and SELFIES with a dot removed')\n",
    "print(cleaned_df_selfies.shape)\n",
    "cleaned_df_selfies.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "473ba9feb7649d7c",
   "metadata": {},
   "source": [
    "## Create a DF with unique SELFIES"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d9b6c102b7bd553",
   "metadata": {},
   "source": [
    "selfies_df = cleaned_df_selfies[[\"SELFIES\"]].drop_duplicates().reset_index(drop=True)\n",
    "print(selfies_df.shape)\n",
    "selfies_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd54469f5bf6eeaf",
   "metadata": {},
   "source": [
    "## Create a dictionary with each SELFIES component"
   ]
  },
  {
   "cell_type": "code",
   "id": "129bda85542afa1b",
   "metadata": {},
   "source": [
    "selfies_group_dict = {}\n",
    "if NEW_RANDOM_DICT:\n",
    "    # extract unique SELFIES groups\n",
    "    unique_selfies_groups = sf.get_alphabet_from_selfies(selfies_df[\"SELFIES\"])\n",
    "\n",
    "    # create the dictionary with enumerated positions\n",
    "    selfies_group_dict = {group: idx+1 for idx, group in enumerate(unique_selfies_groups)}\n",
    "    selfies_group_dict['[nop]'] = 0\n",
    "selfies_group_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8dc95313919a2cb4",
   "metadata": {},
   "source": [
    "if not(NEW_ORDERED_DICT or NEW_RANDOM_DICT):\n",
    "    # read json file into dictionary\n",
    "    with open(f'{base}/Code/Full_systems/Selfies_Mol/Featurization/selfies_group_dict.json', 'r') as json_file:\n",
    "        selfies_group_dict = json.load(json_file)\n",
    "selfies_group_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3bfcd0f58525265d",
   "metadata": {},
   "source": [
    "## Get the frequency of each SELFIES component"
   ]
  },
  {
   "cell_type": "code",
   "id": "df9c5d89a21f4e83",
   "metadata": {},
   "source": [
    "# split SELFIES into fragments and count frequencies\n",
    "all_keys = []\n",
    "for selfies in selfies_df[\"SELFIES\"]:\n",
    "    all_keys.extend(selfies.split(\"][\"))  # split on \"][\" to get individual fragments\n",
    "\n",
    "# normalize fragments by adding brackets back\n",
    "normalized_keys = [f\"[{key.strip('][')}]\" for key in all_keys]\n",
    "\n",
    "# count frequencies\n",
    "key_frequencies = Counter(normalized_keys)\n",
    "\n",
    "# display the frequencies\n",
    "print(f'There are {len(key_frequencies)+1} different selfies parts')\n",
    "key_frequencies\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1bba5ad1e55cac8",
   "metadata": {},
   "source": [
    "if NEW_ORDERED_DICT:\n",
    "    selfies_group_dict = {'[nop]': 0}\n",
    "    c=0\n",
    "    for i in key_frequencies:\n",
    "        c+=1\n",
    "        selfies_group_dict[i] = c\n",
    "\n",
    "    with open(f'{base}/Code/Full_systems/Selfies_Mol/Featurization/selfies_group_dict.json', \"w\") as file:\n",
    "        json.dump(selfies_group_dict, file, indent=4)\n",
    "selfies_group_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4de7b3aab2ed4b59",
   "metadata": {},
   "source": [
    "## Get the frequency of each SELFIES length"
   ]
  },
  {
   "cell_type": "code",
   "id": "319d247c835aa3e0",
   "metadata": {},
   "source": [
    "slefies_length = selfies_df[\"SELFIES\"].apply(lambda x: len(x.split(\"][\")))\n",
    "\n",
    "# count the frequency of each length. Sort by the length\n",
    "length_frequencies = dict(Counter(slefies_length).most_common())\n",
    "length_frequencies"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "396f0b16464689f8",
   "metadata": {},
   "source": [
    "# sorted by the frequency\n",
    "dict(sorted(length_frequencies.items()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd73a709abc642c2",
   "metadata": {},
   "source": [
    "# create a bar chart for the frequencies\n",
    "max_length = max(length_frequencies.keys())\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(length_frequencies.keys(), length_frequencies.values(), width=0.6, label=f'SELFIES length, max Length: {max_length}')\n",
    "\n",
    "# Add thin vertical dotted lines at each x position where there's a bar\n",
    "for bar in bars:\n",
    "    if bar.get_height() > 0:  # Only add a line where there's a bar\n",
    "        plt.axvline(x=bar.get_x() + bar.get_width() / 2, color='red', linestyle='dotted', zorder=0)\n",
    "        \n",
    "\n",
    "min_length = 0\n",
    "plt.xticks(range(min_length, max_length + 1, 10))\n",
    "plt.xlabel('Number of Fragments in SELFIES', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Frequency of SELFIES Lengths', fontsize=14)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8d21f4ded47c9a3",
   "metadata": {},
   "source": [
    "Remove the SELFIES longer than 60. 60 will be the maximum length of the SELFIES"
   ]
  },
  {
   "cell_type": "code",
   "id": "6dd0882239e9a5c",
   "metadata": {},
   "source": [
    "MAX_SELFIES_LENGTH = int(os.getenv(\"MAX_SELFIES_LENGTH\"))\n",
    "MAX_SELFIES_LENGTH"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c01c369e56a47222",
   "metadata": {},
   "source": [
    "# remove rows with SELFIES longer than 60\n",
    "cleaned_df_selfies_cropped = cleaned_df_selfies[cleaned_df_selfies[\"SELFIES\"].apply(lambda x: len(x.split(\"][\")) <= MAX_SELFIES_LENGTH)].reset_index(drop=True)\n",
    "print(cleaned_df_selfies_cropped.shape)\n",
    "print(f'{cleaned_df_selfies.shape[0]-cleaned_df_selfies_cropped.shape[0]} rows removed')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b0e2bc554da6d2e",
   "metadata": {},
   "source": [
    "cleaned_df_selfies_cropped"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8aadc712e17ec381",
   "metadata": {},
   "source": [
    "## Featurize the SELFIES"
   ]
  },
  {
   "cell_type": "code",
   "id": "1c03fd14c660d645",
   "metadata": {},
   "source": [
    "selfies_group_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8684c326de238308",
   "metadata": {},
   "source": [
    "len(selfies_group_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4aaef617f139b70f",
   "metadata": {},
   "source": [
    "hot = sf.batch_selfies_to_flat_hot(cleaned_df_selfies_cropped['SELFIES'].astype(str).tolist(), vocab_stoi=selfies_group_dict, pad_to_len=MAX_SELFIES_LENGTH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40c5d106c84039b7",
   "metadata": {},
   "source": [
    "del cleaned_df_selfies, cleaned_df\n",
    "gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ad7cd461b1351c0",
   "metadata": {},
   "source": [
    "#hot: list of one hot encoding lists\n",
    "print(len(hot), len(hot[0]))\n",
    "#hot[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a2d9d631c242449",
   "metadata": {},
   "source": [
    "# selfies_emb_df = pd.DataFrame(hot, columns=[f'bit_{i}' for i in range(MAX_SELFIES_LENGTH*len(selfies_group_dict))])\n",
    "# print('hot deleted')\n",
    "# selfies_emb_df['SELFIES'] = cleaned_df_selfies_cropped['SELFIES']\n",
    "# print(selfies_emb_df.shape)\n",
    "# selfies_emb_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a78182327d7ce244",
   "metadata": {},
   "source": [
    "# df_cols = [f'bit_{i}' for i in range(MAX_SELFIES_LENGTH*len(selfies_group_dict))]\n",
    "# df_cols.append('SELFIES')\n",
    "# selfies_emb_df = pd.DataFrame(columns=df_cols)\n",
    "# for encoding in tqdm(range(len(hot))):\n",
    "#     row = hot[encoding]\n",
    "#     row.append(cleaned_df_selfies_cropped.iloc[encoding]['SELFIES'])\n",
    "#     selfies_emb_df.loc[encoding] = row"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e55b14722dcbe0e2",
   "metadata": {},
   "source": [
    "# define the columns for the dataframe\n",
    "df_cols = [f'bit_{i}' for i in range(MAX_SELFIES_LENGTH * len(selfies_group_dict))]\n",
    "df_cols.append('SELFIES')\n",
    "\n",
    "# batch size to create dataframes\n",
    "batch_size = 5000\n",
    "\n",
    "# initialize an empty list to hold rows temporarily\n",
    "temp_rows = []\n",
    "\n",
    "# a list to store dataframe batches\n",
    "dataframes = []\n",
    "\n",
    "#selfies_emb_df = pd.DataFrame(columns=df_cols)\n",
    "\n",
    "# iterate over the encodings and process each row\n",
    "for encoding in tqdm(range(len(hot))):\n",
    "\n",
    "    # create the row from hot and append 'SELFIES' value\n",
    "    row = hot[encoding]\n",
    "    row.append(cleaned_df_selfies_cropped.iloc[encoding]['SELFIES'])\n",
    "\n",
    "    # selfies_emb_df.loc[encoding] = row\n",
    "        \n",
    "    # add the row to the temporary list\n",
    "    temp_rows.append(row)\n",
    "    \n",
    "    # when the batch is full, convert it to a DataFrame and store it\n",
    "    if len(temp_rows) == batch_size:\n",
    "        batch_df = pd.DataFrame(temp_rows, columns=df_cols)\n",
    "        dataframes.append(batch_df)\n",
    "        temp_rows = []  # reset the temporary list to save memory\n",
    "\n",
    "# handle any remaining rows after the loop ends\n",
    "if temp_rows:\n",
    "    batch_df = pd.DataFrame(temp_rows, columns=df_cols)\n",
    "    dataframes.append(batch_df)\n",
    "\n",
    "del hot, temp_rows\n",
    "gc.collect()\n",
    "\n",
    "# concatenate all batch dataframes to create the final dataframe\n",
    "selfies_emb_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "selfies_emb_df.to_pickle(f'{os.getcwd()}/one_hot_selfies_encoding.pkl')\n",
    "print('one_hot_selfies_encoding.pkl saved')\n",
    "\n",
    "# display the resulting dataframe\n",
    "print(selfies_emb_df.shape)\n",
    "selfies_emb_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6811a7c7a7bb692f",
   "metadata": {},
   "source": [
    "len(selfies_group_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cec072694155249c",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "id": "7929fed691137ffc",
   "metadata": {},
   "source": [
    "# selfies_emb_unique_df = selfies_emb_df.drop_duplicates().reset_index(drop=True)\n",
    "# print(selfies_emb_unique_df.shape)\n",
    "# selfies_emb_unique_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a37fa9c1f94bc7b6",
   "metadata": {},
   "source": [
    "## Save the featurized SELFIES into a PKL"
   ]
  },
  {
   "cell_type": "code",
   "id": "89f80b08f59a0559",
   "metadata": {},
   "source": [
    "# selfies_emb_unique_df.to_csv(f'{base}/Dataset/Embeddings/{os.getenv(\"SELFIES_EMBEDDING\")}', index=False)\n",
    "# print(f'Saved the SELFIES embeddings into {base}/Dataset/Embeddings/{os.getenv(\"SELFIES_EMBEDDING\")}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5b1276fcefae975",
   "metadata": {},
   "source": [
    "# Combine the SELFIES embeddings with the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "id": "5adf027c5de84641",
   "metadata": {},
   "source": [
    "ms_emb_df = pd.concat([cleaned_df_selfies_cropped, selfies_emb_df.drop(columns=['SELFIES'])], axis=1)\n",
    "print(ms_emb_df.shape)\n",
    "ms_emb_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa609572b9aca10f",
   "metadata": {},
   "source": [
    "## Separate X and y"
   ]
  },
  {
   "cell_type": "code",
   "id": "4772fd99c7773bd5",
   "metadata": {},
   "source": [
    "spectra_columns = [col for col in ms_emb_df.columns if 'mz' in col]\n",
    "X = ms_emb_df[spectra_columns]\n",
    "if NORMALIZE_SPECTRA:\n",
    "    X = X / 999\n",
    "print(X.shape)\n",
    "X"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ccf50600e6dda3bb",
   "metadata": {},
   "source": [
    "embedding_columns = [col for col in ms_emb_df.columns if 'bit_' in col]\n",
    "y = ms_emb_df[embedding_columns]\n",
    "print(y.shape)\n",
    "y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8f92392996efbf4",
   "metadata": {},
   "source": [
    "## Separation into train and test"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ac3ad495dd43ce9",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = NNUtils.divide_big_train_and_test_data(X, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "244942f690aaa956",
   "metadata": {},
   "source": [
    "selfies_test = ms_emb_df.loc[X_test.index, 'SELFIES']\n",
    "selfies_test.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d2325a02dd6a317",
   "metadata": {},
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e06dbe72a03ad529",
   "metadata": {},
   "source": [
    "print(y_train.shape)\n",
    "y_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e86ac5ad8dc71928",
   "metadata": {},
   "source": [
    "print(X_test.shape)\n",
    "X_test.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66a15c7dfab81a14",
   "metadata": {},
   "source": [
    "print(y_test.shape)\n",
    "y_test.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a59abd3971069bce",
   "metadata": {},
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "selfies_test.reset_index(drop=True, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdadb48ffab6881b",
   "metadata": {},
   "source": [
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "input_size, output_size"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1b5294c06b7c6f",
   "metadata": {},
   "source": [
    "X_train.to_pickle(os.getenv('X_TRAIN'))\n",
    "print(f\"{os.getenv('X_TRAIN')} saved\")\n",
    "y_train.to_pickle(os.getenv('Y_TRAIN'))\n",
    "print(f\"{os.getenv('Y_TRAIN')} saved\")\n",
    "X_test.to_pickle(os.getenv('X_TEST'))\n",
    "print(f\"{os.getenv('X_TEST')} saved\")\n",
    "y_test.to_pickle(os.getenv('Y_TEST'))\n",
    "print(f\"{os.getenv('Y_TEST')} saved\")\n",
    "selfies_test.to_pickle(os.getenv('SELFIES_X_TEST'))\n",
    "print(f\"{os.getenv('SELFIES_X_TEST')} saved\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2a61a6822824538",
   "metadata": {},
   "source": [
    "print('done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4128b4b2-3e5e-44d7-b7b1-3d6d66060957",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
